{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "By \n",
    "\n",
    "A. Ntoumi & A. Steger (University of Groningen, Language Technology Project 2019-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, re, string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score, hamming_loss, precision_score, recall_score\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stegerakos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/stegerakos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = SnowballStemmer(\"dutch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Authors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set column names, since they're not setted in the txt file\n",
    "column_names = [\"user_id\", \"dob\", \"gender\", \"polarity\", \"city\", \"country\", \"personality\", \"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv(\"csicorpus/List.CSI.AuthorData.1.4.0.BV.2016-02-08.txt\", sep=\"\\t\", names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>polarity</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>personality</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60235486</td>\n",
       "      <td>1990</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antwerpen</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>----</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43931362</td>\n",
       "      <td>1991</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antwerpen</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>----</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11987873</td>\n",
       "      <td>1988-17-04</td>\n",
       "      <td>Male</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Antwerpen</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>93-30-53-32-22</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98696422</td>\n",
       "      <td>1986-26-07</td>\n",
       "      <td>Male</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Antwerpen</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>90-47-48-22-37</td>\n",
       "      <td>I44-N50-T01-J11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36964375</td>\n",
       "      <td>1990</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antwerpen</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>----</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id         dob  gender  polarity       city  country     personality  \\\n",
       "0  60235486        1990  Female       NaN  Antwerpen  Belgium            ----   \n",
       "1  43931362        1991  Female       NaN  Antwerpen  Belgium            ----   \n",
       "2  11987873  1988-17-04    Male  Straight  Antwerpen  Belgium  93-30-53-32-22   \n",
       "3  98696422  1986-26-07    Male  Straight  Antwerpen  Belgium  90-47-48-22-37   \n",
       "4  36964375        1990  Female       NaN  Antwerpen  Belgium            ----   \n",
       "\n",
       "             other  \n",
       "0              ---  \n",
       "1              ---  \n",
       "2              ---  \n",
       "3  I44-N50-T01-J11  \n",
       "4              ---  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unify the authors datafarame so that we got no duplicate\n",
    "authors.drop_duplicates(subset='user_id', keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the essays data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_id(filename):\n",
    "    \"\"\" \n",
    "    This is a function that extracts the user_id from a given filename \n",
    "    \n",
    "    Parameters :\n",
    "    - filename (string) : # 59260694_Essay_2014-06-01.txt\n",
    "    \n",
    "    Returns:\n",
    "    - user_id (string) : 5926069\n",
    "    \"\"\"\n",
    "    return filename.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'59260694'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_user_id(\"59260694_Essay_2014-06-01.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the essay dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the essays path\n",
    "path = \"./csicorpus/essays/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a new empty data list \n",
    "data_list = []\n",
    "\n",
    "#loop over the list of txt files to read their data\n",
    "for filename in os.listdir(path):\n",
    "    if filename != \".ipynb_checkpoints\":\n",
    "        # read the txt file inside\n",
    "        data = open(path+filename, \"r\").read()\n",
    "        data_list.append(\n",
    "            {\n",
    "                'user_id' : extract_user_id(filename),\n",
    "                'filename': filename,\n",
    "                'essay': data\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final dataframe\n",
    "essays_df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>filename</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Het orgaandonorschap: een dualistische visie\\n...</td>\n",
       "      <td>92419385_Essay_2015-01-06.txt</td>\n",
       "      <td>92419385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iedereen automatisch geregistreerd als donor, ...</td>\n",
       "      <td>89353110_Essay_2015-01-06.txt</td>\n",
       "      <td>89353110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gezond leven: het middel om langer te leven?\\n...</td>\n",
       "      <td>30423159_Essay_2012.txt</td>\n",
       "      <td>30423159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿Inleiding\\nWe kunnen er niet omheen dat het E...</td>\n",
       "      <td>95257163_Paper_2014-06-01.txt</td>\n",
       "      <td>95257163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Enkele jaren geleden was men er zeker van dat ...</td>\n",
       "      <td>34322552_Essay_2013-06-01.txt</td>\n",
       "      <td>34322552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay  \\\n",
       "0  Het orgaandonorschap: een dualistische visie\\n...   \n",
       "1  Iedereen automatisch geregistreerd als donor, ...   \n",
       "2  Gezond leven: het middel om langer te leven?\\n...   \n",
       "3  ﻿Inleiding\\nWe kunnen er niet omheen dat het E...   \n",
       "4  Enkele jaren geleden was men er zeker van dat ...   \n",
       "\n",
       "                        filename   user_id  \n",
       "0  92419385_Essay_2015-01-06.txt  92419385  \n",
       "1  89353110_Essay_2015-01-06.txt  89353110  \n",
       "2        30423159_Essay_2012.txt  30423159  \n",
       "3  95257163_Paper_2014-06-01.txt  95257163  \n",
       "4  34322552_Essay_2013-06-01.txt  34322552  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the built-in dataframe\n",
    "essays_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 3 columns):\n",
      "essay       517 non-null object\n",
      "filename    517 non-null object\n",
      "user_id     517 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 12.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# get some insight from the resulted dataframe\n",
    "essays_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we cen see from the information above, we have exactly around 517 observations, without null values or missed ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join all the two dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted all the two dataframes, we proceed to join them by `user_id`, so that for each `user_id` we will keep only their `essays` and `personality`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join between essays and reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the user_id type from int to object, so that we can join them between the same columns type \n",
    "authors.user_id = authors.user_id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join Authors and df_inner\n",
    "df = pd.merge(essays_df[[\"user_id\", \"essay\"]], authors[[\"user_id\", \"personality\"]], on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98634123</td>\n",
       "      <td>Voetbal is internationaal een zeer geliefde sp...</td>\n",
       "      <td>24-30-53-74-80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98634123</td>\n",
       "      <td>Kan een kind met cochleair implantaat zijn taa...</td>\n",
       "      <td>24-30-53-74-80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98634123</td>\n",
       "      <td>Om meer vrouwen aan de top van het bedrijfslev...</td>\n",
       "      <td>24-30-53-74-80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81842160</td>\n",
       "      <td>\"Iedereen automatisch geregistreerd als donor,...</td>\n",
       "      <td>41-79-53-17-84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81842160</td>\n",
       "      <td>Opgelet: mama en papa lezen mee\\nTegenwoordig ...</td>\n",
       "      <td>41-79-53-17-84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id                                              essay     personality\n",
       "0  98634123  Voetbal is internationaal een zeer geliefde sp...  24-30-53-74-80\n",
       "1  98634123  Kan een kind met cochleair implantaat zijn taa...  24-30-53-74-80\n",
       "2  98634123  Om meer vrouwen aan de top van het bedrijfslev...  24-30-53-74-80\n",
       "3  81842160  \"Iedereen automatisch geregistreerd als donor,...  41-79-53-17-84\n",
       "4  81842160  Opgelet: mama en papa lezen mee\\nTegenwoordig ...  41-79-53-17-84"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the final raw dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding two new columns to the existing dataframe. \n",
    "# by default splitting is done on the basis of single space\n",
    "df[['Openness','Conscientiousness', 'Extroversion', 'Agreeableness', 'Neuroticism']] = df.personality.str.split(pat=\"-\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>personality</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Extroversion</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Neuroticism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98634123</td>\n",
       "      <td>Voetbal is internationaal een zeer geliefde sp...</td>\n",
       "      <td>24-30-53-74-80</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98634123</td>\n",
       "      <td>Kan een kind met cochleair implantaat zijn taa...</td>\n",
       "      <td>24-30-53-74-80</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98634123</td>\n",
       "      <td>Om meer vrouwen aan de top van het bedrijfslev...</td>\n",
       "      <td>24-30-53-74-80</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81842160</td>\n",
       "      <td>\"Iedereen automatisch geregistreerd als donor,...</td>\n",
       "      <td>41-79-53-17-84</td>\n",
       "      <td>41</td>\n",
       "      <td>79</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81842160</td>\n",
       "      <td>Opgelet: mama en papa lezen mee\\nTegenwoordig ...</td>\n",
       "      <td>41-79-53-17-84</td>\n",
       "      <td>41</td>\n",
       "      <td>79</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59260694</td>\n",
       "      <td>Beperkt de vrijheid ons of maken beperkingen o...</td>\n",
       "      <td>----</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id                                              essay  \\\n",
       "0  98634123  Voetbal is internationaal een zeer geliefde sp...   \n",
       "1  98634123  Kan een kind met cochleair implantaat zijn taa...   \n",
       "2  98634123  Om meer vrouwen aan de top van het bedrijfslev...   \n",
       "3  81842160  \"Iedereen automatisch geregistreerd als donor,...   \n",
       "4  81842160  Opgelet: mama en papa lezen mee\\nTegenwoordig ...   \n",
       "5  59260694  Beperkt de vrijheid ons of maken beperkingen o...   \n",
       "\n",
       "      personality Openness Conscientiousness Extroversion Agreeableness  \\\n",
       "0  24-30-53-74-80       24                30           53            74   \n",
       "1  24-30-53-74-80       24                30           53            74   \n",
       "2  24-30-53-74-80       24                30           53            74   \n",
       "3  41-79-53-17-84       41                79           53            17   \n",
       "4  41-79-53-17-84       41                79           53            17   \n",
       "5            ----                                                         \n",
       "\n",
       "  Neuroticism  \n",
       "0          80  \n",
       "1          80  \n",
       "2          80  \n",
       "3          84  \n",
       "4          84  \n",
       "5              "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove rows where personality is empty\n",
    "df = df[df.personality != \"----\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 470 entries, 0 to 474\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   user_id            470 non-null    object\n",
      " 1   essay              470 non-null    object\n",
      " 2   personality        470 non-null    object\n",
      " 3   Openness           470 non-null    object\n",
      " 4   Conscientiousness  470 non-null    object\n",
      " 5   Extroversion       470 non-null    object\n",
      " 6   Agreeableness      470 non-null    object\n",
      " 7   Neuroticism        470 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 33.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the final datafarame within `essays` and the user personality scores, let's move forward to do feature engineering and Data modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stopword list\n",
    "stoplist = stopwords.words('dutch') \n",
    "# get list of punctuations\n",
    "punctuations = string.punctuation + \"’¶•@°©®™\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    This function preprocesses a given raw text by removing the urls, mentions,\n",
    "    punctuations, stop words, numbers, emojis etc.\n",
    "    \n",
    "    @param text string\n",
    "    @return text string\n",
    "    \"\"\"\n",
    "        \n",
    "    # string to lowercase\n",
    "    txt = text.lower()\n",
    "    \n",
    "    # keep only ascii characters\n",
    "    txt = re.sub(r\"[^a-zA-ZÀ-ÿ]\", \" \", txt)\n",
    "    \n",
    "    # punctuation removal and map it to space\n",
    "    translator = str.maketrans(punctuations, \" \"*len(punctuations))\n",
    "    s = txt.translate(translator)\n",
    "    \n",
    "    # remove digits \n",
    "    no_digits = ''.join([i for i in s if not i.isdigit()])\n",
    "    cleaner = \" \".join(no_digits.split())\n",
    "    \n",
    "    # tokenize words and removing stop words \n",
    "    word_tokens = word_tokenize(cleaner)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stoplist]\n",
    "    filtered_sentence = \" \".join(filtered_sentence)\n",
    "    \n",
    "    # a stemming word block\n",
    "    filtered_sentence = [stemmer.stem(word) for word in word_tokenize(filtered_sentence)]\n",
    "    filtered_sentence = \" \".join(filtered_sentence)\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we are dealing with textual data\n",
    "df.essay = df.essay.astype(str)\n",
    "# apply the preprocessing function to clean up the text data\n",
    "df[\"clean_essay\"] = df.essay.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>personality</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Extroversion</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>clean_essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>22966612</td>\n",
       "      <td>1.\\tINLEIDING\\nDe geschiedschrijving van de st...</td>\n",
       "      <td>84-79-22-69-43</td>\n",
       "      <td>84</td>\n",
       "      <td>79</td>\n",
       "      <td>22</td>\n",
       "      <td>69</td>\n",
       "      <td>43</td>\n",
       "      <td>inleid geschiedschrijv stedelijk nederland lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>99317183</td>\n",
       "      <td>VROUW BREEKT NIET DOOR GLAZEN PLAFOND\\n\\n ",
       "Rece...</td>\n",
       "      <td>35-97-97-38-4</td>\n",
       "      <td>35</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>vrouw breekt glaz plafond recent onderzoek ku ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>89263341</td>\n",
       "      <td>Toon je groot hart, word donor (Kato De Maerte...</td>\n",
       "      <td>76-13-55-3-43</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>ton grot hart word donor kato maertelaer stel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>35362991</td>\n",
       "      <td>Geluk maak je zelf, dus stop met dat geklaag.\\...</td>\n",
       "      <td>65-10-3-3-84</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>84</td>\n",
       "      <td>geluk mak stop geklag jar gaf vlaming gelukk a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>90392198</td>\n",
       "      <td>Biofundamentalisme: een angstkreet\\nBoodschapp...</td>\n",
       "      <td>84-74-42-57-80</td>\n",
       "      <td>84</td>\n",
       "      <td>74</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>80</td>\n",
       "      <td>biofundamentalism angstkret boodschapp doe nat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                              essay  \\\n",
       "125  22966612  1.\\tINLEIDING\\nDe geschiedschrijving van de st...   \n",
       "334  99317183  VROUW BREEKT NIET DOOR GLAZEN PLAFOND\\n\\n\n",
       "Rece...   \n",
       "403  89263341  Toon je groot hart, word donor (Kato De Maerte...   \n",
       "469  35362991  Geluk maak je zelf, dus stop met dat geklaag.\\...   \n",
       "390  90392198  Biofundamentalisme: een angstkreet\\nBoodschapp...   \n",
       "\n",
       "        personality Openness Conscientiousness Extroversion Agreeableness  \\\n",
       "125  84-79-22-69-43       84                79           22            69   \n",
       "334   35-97-97-38-4       35                97           97            38   \n",
       "403   76-13-55-3-43       76                13           55             3   \n",
       "469    65-10-3-3-84       65                10            3             3   \n",
       "390  84-74-42-57-80       84                74           42            57   \n",
       "\n",
       "    Neuroticism                                        clean_essay  \n",
       "125          43  inleid geschiedschrijv stedelijk nederland lan...  \n",
       "334           4  vrouw breekt glaz plafond recent onderzoek ku ...  \n",
       "403          43  ton grot hart word donor kato maertelaer stel ...  \n",
       "469          84  geluk mak stop geklag jar gaf vlaming gelukk a...  \n",
       "390          80  biofundamentalism angstkret boodschapp doe nat...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking data after text preprocessing\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding of the `personality` features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will encode the personality features to a binary 0 and 1, based on a standard threshold of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a list of personalities\n",
    "personalities = ['Openness', 'Conscientiousness', 'Extroversion', 'Agreeableness', 'Neuroticism']\n",
    "# loop over them and replace values\n",
    "for column in personalities:\n",
    "    # convert them to int first\n",
    "    df[column] = df[column].astype(int)\n",
    "    # replace values less than 50 with 0\n",
    "    df.loc[df[column] < 50, column] = 0\n",
    "    # replace values over than 50 with 1\n",
    "    df.loc[df[column] >= 50, column] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>personality</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Extroversion</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>clean_essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>83325638</td>\n",
       "      <td>werkloosheidsuitkeringen stoppen is een onmen...</td>\n",
       "      <td>35-17-42-69-0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>werkloosheidsuitker stopp onmens straf nva bep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>30158895</td>\n",
       "      <td>Geluk en de Belgen\\n\\nTegenwooordig zullen mee...</td>\n",
       "      <td>53-52-74-57-90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>geluk belg tegenwooord zull mens bekenn ongelu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>78258514</td>\n",
       "      <td>De laatste jaren is het voor de gemiddelde men...</td>\n",
       "      <td>47-3-12-22-7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>laatst jar gemiddeld men sted makkelijker gewo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>62557275</td>\n",
       "      <td>Werkloosheidsuitkeringen moeten beperkt worden...</td>\n",
       "      <td>59-6-79-38-55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>werkloosheidsuitker moet beperkt tijd laatst m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>21002950</td>\n",
       "      <td>Werkloosheidsuitkeringen moeten beperkt worden...</td>\n",
       "      <td>84-47-70-8-80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>werkloosheidsuitker moet beperkt tijd werkloos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                              essay  \\\n",
       "325  83325638   werkloosheidsuitkeringen stoppen is een onmen...   \n",
       "212  30158895  Geluk en de Belgen\\n\\nTegenwooordig zullen mee...   \n",
       "448  78258514  De laatste jaren is het voor de gemiddelde men...   \n",
       "145  62557275  Werkloosheidsuitkeringen moeten beperkt worden...   \n",
       "411  21002950  Werkloosheidsuitkeringen moeten beperkt worden...   \n",
       "\n",
       "        personality  Openness  Conscientiousness  Extroversion  Agreeableness  \\\n",
       "325   35-17-42-69-0         0                  0             0              1   \n",
       "212  53-52-74-57-90         1                  1             1              1   \n",
       "448    47-3-12-22-7         0                  0             0              0   \n",
       "145   59-6-79-38-55         1                  0             1              0   \n",
       "411   84-47-70-8-80         1                  0             1              0   \n",
       "\n",
       "     Neuroticism                                        clean_essay  \n",
       "325            0  werkloosheidsuitker stopp onmens straf nva bep...  \n",
       "212            1  geluk belg tegenwooord zull mens bekenn ongelu...  \n",
       "448            0  laatst jar gemiddeld men sted makkelijker gewo...  \n",
       "145            1  werkloosheidsuitker moet beperkt tijd laatst m...  \n",
       "411            1  werkloosheidsuitker moet beperkt tijd werkloos...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the sample data again\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the data for next time use cases (LSTM)\n",
    "df.to_csv(\"./data/clean_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in train & test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will split the data into training and test set, to build our SVM model that will predict the participant's personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into train and test\n",
    "training, test = train_test_split(df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 376 obseravtion in training set, and 94 for test set\n"
     ]
    }
   ],
   "source": [
    "print(\"We have {} obseravtion in training set, and {} for test set\".format(len(training), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the training splits\n",
    "train_x = [x for x in training.clean_essay]\n",
    "\n",
    "train_y_EXT = [x for x in training.Extroversion]\n",
    "train_y_NEU = [x for x in training.Neuroticism]\n",
    "train_y_AGR = [x for x in training.Agreeableness]\n",
    "train_y_CON = [x for x in training.Conscientiousness]\n",
    "train_y_OPN = [x for x in training.Openness]\n",
    "\n",
    "# preparing the test splits\n",
    "test_x = [x for x in test.clean_essay]\n",
    "\n",
    "test_y_EXT = [x for x in test.Extroversion]\n",
    "test_y_NEU = [x for x in test.Neuroticism]\n",
    "test_y_AGR = [x for x in test.Agreeableness]\n",
    "test_y_CON = [x for x in test.Conscientiousness]\n",
    "test_y_OPN = [x for x in test.Openness]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a new tfidf vectorizer object\n",
    "tfidf__vectorizer = TfidfVectorizer()\n",
    "\n",
    "# transform our corpus to a tfidf matrix\n",
    "train_x_vectors = tfidf__vectorizer.fit_transform(train_x)\n",
    "test_x_vectors = tfidf__vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to save the tf-idf vocabulary so we can use it next time with any given document, without needing to load the data at each time and transform it to a tf-idf matrix so we can map our text to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tfidf vocabulary into a pickle file, so wan use it next time\n",
    "# to execute only once, after it's saved you can load it directly\n",
    "#pickle.dump(tfidf__vectorizer.vocabulary_, open(\"./tfidf_vocab/data_vocab.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our model, first we need to do some hyperparameter tuning in order to extract the best parameters to maximize the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {\n",
    "    'C': [1,2,3,4,5,6,7,8,9,10,11,12], \n",
    "    'kernel': ['linear','rbf']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extroversion classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 s, sys: 292 ms, total: 1.3 s\n",
      "Wall time: 4.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get an instance of the SVM model\n",
    "EXT_model = svm.SVC()\n",
    "\n",
    "# Making models with hyper parameters sets\n",
    "CV_svc = GridSearchCV(EXT_model, param_grid=svm_params, n_jobs=-1, cv=5, scoring=\"f1_micro\")\n",
    "\n",
    "# fitting the model\n",
    "CV_svc.fit(train_x_vectors, train_y_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters is : {'C': 2, 'kernel': 'rbf'}\n",
      "Best f1-score is : 0.5583859649122808 \n"
     ]
    }
   ],
   "source": [
    "# The best hyperparameter set\n",
    "print(\"Best Hyper Parameters is : {}\".format(CV_svc.best_params_))\n",
    "print(\"Best f1-score is : {} \".format(CV_svc.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SVM model based on these best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Extraversion EXT model with SVM and tf-idf ...\n",
      "Extroversion detection accuracy score:  0.5319148936170213\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Extraversion EXT model with SVM and tf-idf ...\")\n",
    "\n",
    "clf_svm_EXT = CV_svc.best_estimator_\n",
    "clf_svm_EXT.fit(train_x_vectors, train_y_EXT)\n",
    "\n",
    "print(\"Extroversion detection accuracy score: \", clf_svm_EXT.score(test_x_vectors, test_y_EXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-average quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro : 0.5319\n",
      "Hamming loss : 0.4681\n",
      "Precision : 0.5319\n",
      "Recall : 0.5319\n"
     ]
    }
   ],
   "source": [
    "# get the predictions over the test observations\n",
    "y_pred = clf_svm_EXT.predict(test_x_vectors)\n",
    "\n",
    "print(\"F1-micro : {:.4f}\".format(f1_score(test_y_EXT, y_pred, average='micro')))\n",
    "print(\"Hamming loss : {:.4f}\".format(hamming_loss(test_y_EXT,y_pred)))\n",
    "print(\"Precision : {:.4f}\".format(precision_score(test_y_EXT, y_pred, average='micro')))\n",
    "print(\"Recall : {:.4f}\".format(recall_score(test_y_EXT, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.21      0.29        42\n",
      "           1       0.55      0.79      0.65        52\n",
      "\n",
      "    accuracy                           0.53        94\n",
      "   macro avg       0.50      0.50      0.47        94\n",
      "weighted avg       0.51      0.53      0.49        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report of the model\n",
    "print(classification_report(test_y_EXT, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the model for next time predictions\n",
    "model_path = \"./models/EXT_svm_model.pkl\"  \n",
    "with open(model_path, 'wb') as file:  \n",
    "    pickle.dump(clf_svm_EXT, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neuroticism classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 268 ms, total: 1.28 s\n",
      "Wall time: 4.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get an instance of the SVM model\n",
    "NEU_model = svm.SVC()\n",
    "\n",
    "# Making models with hyperparameter set\n",
    "CV_svc = GridSearchCV(NEU_model, param_grid=svm_params, n_jobs=-1, cv=5, scoring=\"f1_micro\")\n",
    "\n",
    "# fitting the model\n",
    "CV_svc.fit(train_x_vectors, train_y_NEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters is : {'C': 5, 'kernel': 'linear'}\n",
      "Best f1-score is : 0.5025964912280702 \n"
     ]
    }
   ],
   "source": [
    "# The best hyperparameter set\n",
    "print(\"Best Hyper Parameters is : {}\".format(CV_svc.best_params_))\n",
    "print(\"Best f1-score is : {} \".format(CV_svc.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SVM model based on these best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neuroticism NEU model with SVM and tf-idf ...\n",
      "Neuroticism detection accuracy score:  0.5106382978723404\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Neuroticism NEU model with SVM and tf-idf ...\")\n",
    "\n",
    "clf_svm_NEU = CV_svc.best_estimator_\n",
    "clf_svm_NEU.fit(train_x_vectors, train_y_NEU)\n",
    "\n",
    "print(\"Neuroticism detection accuracy score: \", clf_svm_NEU.score(test_x_vectors, test_y_NEU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-average quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro : 0.5106\n",
      "Hamming loss : 0.4894\n",
      "Precision : 0.5106\n",
      "Recall : 0.5106\n"
     ]
    }
   ],
   "source": [
    "# get the predictions over the test observations\n",
    "y_pred = clf_svm_NEU.predict(test_x_vectors)\n",
    "\n",
    "print(\"F1-micro : {:.4f}\".format(f1_score(test_y_NEU, y_pred, average='micro')))\n",
    "print(\"Hamming loss : {:.4f}\".format(hamming_loss(test_y_NEU,y_pred)))\n",
    "print(\"Precision : {:.4f}\".format(precision_score(test_y_NEU, y_pred, average='micro')))\n",
    "print(\"Recall : {:.4f}\".format(recall_score(test_y_NEU, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.43      0.44        42\n",
      "           1       0.56      0.58      0.57        52\n",
      "\n",
      "    accuracy                           0.51        94\n",
      "   macro avg       0.50      0.50      0.50        94\n",
      "weighted avg       0.51      0.51      0.51        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report of the model\n",
    "print(classification_report(test_y_NEU, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the model for next time predictions\n",
    "model_path = \"./models/NEU_svm_model.pkl\"  \n",
    "with open(model_path, 'wb') as file:  \n",
    "    pickle.dump(clf_svm_NEU, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Openness classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 284 ms, total: 1.29 s\n",
      "Wall time: 4.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get an instance of the SVM model\n",
    "OPN_model = svm.SVC()\n",
    "\n",
    "# Making models with hyperparameter set\n",
    "CV_svc = GridSearchCV(OPN_model, param_grid=svm_params, n_jobs=-1, cv=5, scoring=\"f1_micro\")\n",
    "\n",
    "# fitting the model\n",
    "CV_svc.fit(train_x_vectors, train_y_OPN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters is : {'C': 3, 'kernel': 'rbf'}\n",
      "Best f1-score is : 0.5747017543859648 \n"
     ]
    }
   ],
   "source": [
    "# The best hyperparameter set\n",
    "print(\"Best Hyper Parameters is : {}\".format(CV_svc.best_params_))\n",
    "print(\"Best f1-score is : {} \".format(CV_svc.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SVM model based on these best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Openness OPN model with SVM and tf-idf ...\n",
      "Openness detection accuracy score:  0.48936170212765956\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Openness OPN model with SVM and tf-idf ...\")\n",
    "\n",
    "clf_svm_OPN = CV_svc.best_estimator_\n",
    "clf_svm_OPN.fit(train_x_vectors, train_y_OPN)\n",
    "\n",
    "print(\"Openness detection accuracy score: \", clf_svm_OPN.score(test_x_vectors, test_y_OPN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-average quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro : 0.4894\n",
      "Hamming loss : 0.5106\n",
      "Precision : 0.4894\n",
      "Recall : 0.4894\n"
     ]
    }
   ],
   "source": [
    "# get the predictions over the test observations\n",
    "y_pred = clf_svm_OPN.predict(test_x_vectors)\n",
    "\n",
    "print(\"F1-micro : {:.4f}\".format(f1_score(test_y_OPN, y_pred, average='micro')))\n",
    "print(\"Hamming loss : {:.4f}\".format(hamming_loss(test_y_OPN,y_pred)))\n",
    "print(\"Precision : {:.4f}\".format(precision_score(test_y_OPN, y_pred, average='micro')))\n",
    "print(\"Recall : {:.4f}\".format(recall_score(test_y_OPN, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.45      0.44        42\n",
      "           1       0.54      0.52      0.53        52\n",
      "\n",
      "    accuracy                           0.49        94\n",
      "   macro avg       0.49      0.49      0.49        94\n",
      "weighted avg       0.49      0.49      0.49        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report of the model\n",
    "print(classification_report(test_y_OPN, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the model for next time predictions\n",
    "model_path = \"./models/OPN_svm_model.pkl\"  \n",
    "with open(model_path, 'wb') as file:  \n",
    "    pickle.dump(clf_svm_OPN, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conscientiousness classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 284 ms, total: 1.3 s\n",
      "Wall time: 4.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get an instance of the SVM model\n",
    "CON_model = svm.SVC()\n",
    "\n",
    "# Making models with hyperparameter set\n",
    "CV_svc = GridSearchCV(CON_model, param_grid=svm_params, n_jobs=-1, cv=5, scoring=\"f1_micro\")\n",
    "\n",
    "# fitting the model\n",
    "CV_svc.fit(train_x_vectors, train_y_CON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters is : {'C': 2, 'kernel': 'rbf'}\n",
      "Best f1-score is : 0.5505263157894736 \n"
     ]
    }
   ],
   "source": [
    "# The best hyperparameter set\n",
    "print(\"Best Hyper Parameters is : {}\".format(CV_svc.best_params_))\n",
    "print(\"Best f1-score is : {} \".format(CV_svc.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SVM model based on these best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Conscientiousness CON model with SVM and tf-idf ...\n",
      "Conscientiousness detection accuracy score:  0.5212765957446809\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Conscientiousness CON model with SVM and tf-idf ...\")\n",
    "\n",
    "clf_svm_CON = CV_svc.best_estimator_\n",
    "clf_svm_CON.fit(train_x_vectors, train_y_CON)\n",
    "\n",
    "print(\"Conscientiousness detection accuracy score: \", clf_svm_CON.score(test_x_vectors, test_y_CON))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-average quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro : 0.5213\n",
      "Hamming loss : 0.4787\n",
      "Precision : 0.5213\n",
      "Recall : 0.5213\n"
     ]
    }
   ],
   "source": [
    "# get the predictions over the test observations\n",
    "y_pred = clf_svm_CON.predict(test_x_vectors)\n",
    "\n",
    "print(\"F1-micro : {:.4f}\".format(f1_score(test_y_CON, y_pred, average='micro')))\n",
    "print(\"Hamming loss : {:.4f}\".format(hamming_loss(test_y_CON,y_pred)))\n",
    "print(\"Precision : {:.4f}\".format(precision_score(test_y_CON, y_pred, average='micro')))\n",
    "print(\"Recall : {:.4f}\".format(recall_score(test_y_CON, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.50      0.54        52\n",
      "           1       0.47      0.55      0.51        42\n",
      "\n",
      "    accuracy                           0.52        94\n",
      "   macro avg       0.52      0.52      0.52        94\n",
      "weighted avg       0.53      0.52      0.52        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report of the model\n",
    "print(classification_report(test_y_CON, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the model for next time predictions\n",
    "model_path = \"./models/CON_svm_model.pkl\"  \n",
    "with open(model_path, 'wb') as file:  \n",
    "    pickle.dump(clf_svm_CON, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Agreeableness classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 972 ms, sys: 344 ms, total: 1.32 s\n",
      "Wall time: 4.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get an instance of the SVM model\n",
    "AGR_model = svm.SVC()\n",
    "\n",
    "# Making models with hyperparameter set\n",
    "CV_svc = GridSearchCV(AGR_model, param_grid=svm_params, n_jobs=-1, cv=5, scoring=\"f1_micro\")\n",
    "\n",
    "# fitting the model\n",
    "CV_svc.fit(train_x_vectors, train_y_AGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters is : {'C': 1, 'kernel': 'rbf'}\n",
      "Best f1-score is : 0.547859649122807 \n"
     ]
    }
   ],
   "source": [
    "# The best hyperparameter set\n",
    "print(\"Best Hyper Parameters is : {}\".format(CV_svc.best_params_))\n",
    "print(\"Best f1-score is : {} \".format(CV_svc.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SVM model based on these best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Agreeableness AGR model with SVM and tf-idf ...\n",
      "Agreeableness detection accuracy score:  0.4787234042553192\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Agreeableness AGR model with SVM and tf-idf ...\")\n",
    "\n",
    "clf_svm_AGR = CV_svc.best_estimator_\n",
    "clf_svm_AGR.fit(train_x_vectors, train_y_AGR)\n",
    "\n",
    "print(\"Agreeableness detection accuracy score: \", clf_svm_AGR.score(test_x_vectors, test_y_AGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-average quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-micro : 0.4787\n",
      "Hamming loss : 0.5213\n",
      "Precision : 0.4787\n",
      "Recall : 0.4787\n"
     ]
    }
   ],
   "source": [
    "# get the predictions over the test observations\n",
    "y_pred = clf_svm_AGR.predict(test_x_vectors)\n",
    "\n",
    "print(\"F1-micro : {:.4f}\".format(f1_score(test_y_AGR, y_pred, average='micro')))\n",
    "print(\"Hamming loss : {:.4f}\".format(hamming_loss(test_y_AGR,y_pred)))\n",
    "print(\"Precision : {:.4f}\".format(precision_score(test_y_AGR, y_pred, average='micro')))\n",
    "print(\"Recall : {:.4f}\".format(recall_score(test_y_AGR, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.69      0.62        58\n",
      "           1       0.22      0.14      0.17        36\n",
      "\n",
      "    accuracy                           0.48        94\n",
      "   macro avg       0.39      0.41      0.39        94\n",
      "weighted avg       0.43      0.48      0.45        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report of the model\n",
    "print(classification_report(test_y_AGR, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the model for next time predictions\n",
    "model_path = \"./models/AGR_svm_model.pkl\"  \n",
    "with open(model_path, 'wb') as file:  \n",
    "    pickle.dump(clf_svm_AGR, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the 5 built-in models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the tf-idf vocabulary, we can use it next time with any given document, without needing to load the data at each time and transform it to a tf-idf matrix so we can fit our text to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary later\n",
    "loaded_doc_vec = TfidfVectorizer(decode_error=\"replace\", vocabulary=pickle.load(open(\"./tfidf_vocab/data_vocab.pkl\", \"rb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have saved and loaded our vocabulary, we need to build a function taht will take as parameter a raw text to preprocess it and map it to the built-in vocabulary so we can call our trained models to extract the right personalities from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_five_personality_traits(essay):\n",
    "    # set the constant path variables \n",
    "    path = \"./models/\"\n",
    "    suffix = \"_svm_model.pkl\"\n",
    "    \n",
    "    # first let us load the built-in SVM models\n",
    "    with open(path+\"AGR\"+suffix, 'rb') as file:  \n",
    "        AGR_model = pickle.load(file)\n",
    "    \n",
    "    with open(path+\"CON\"+suffix, 'rb') as file:  \n",
    "        CON_model = pickle.load(file)\n",
    "    \n",
    "    with open(path+\"EXT\"+suffix, 'rb') as file:  \n",
    "        EXT_model = pickle.load(file)\n",
    "    \n",
    "    with open(path+\"NEU\"+suffix, 'rb') as file:  \n",
    "        NEU_model = pickle.load(file)\n",
    "    \n",
    "    with open(path+\"OPN\"+suffix, 'rb') as file:  \n",
    "        OPN_model = pickle.load(file)\n",
    "    \n",
    "    # second, we need to build the feature matrix\n",
    "    transformer = TfidfTransformer()\n",
    "    features = transformer.fit_transform(loaded_doc_vec.fit_transform([preprocess_text(essay)]))\n",
    "    \n",
    "    # now that we have the features, we will get the predictions of the 5 models\n",
    "    AGR_predictions = int(AGR_model.predict(features.toarray()))\n",
    "    CON_prediction = int(CON_model.predict(features.toarray()))\n",
    "    EXT_prediction = int(EXT_model.predict(features.toarray()))\n",
    "    NEU_prediction = int(NEU_model.predict(features.toarray()))\n",
    "    OPN_prediction = int(OPN_model.predict(features.toarray()))\n",
    "    \n",
    "    # build the final dictionary with prediction\n",
    "    final_prediction = {\n",
    "        \"Openness\": OPN_prediction,\n",
    "        \"Conscientiousness\": CON_prediction,\n",
    "        \"Extroversion\": EXT_prediction,\n",
    "        \"Agreeableness\": AGR_predictions,\n",
    "        \"Neuroticism\": NEU_prediction\n",
    "    }\n",
    "    \n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Agreeableness': 1,\n",
      " 'Conscientiousness': 0,\n",
      " 'Extroversion': 1,\n",
      " 'Neuroticism': 1,\n",
      " 'Openness': 1}\n"
     ]
    }
   ],
   "source": [
    "# let us get a text as a sample to test on\n",
    "test_text = df.essay[22]\n",
    "# run predictions over that text\n",
    "res = predict_five_personality_traits(test_text)\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above we proposed a possible solution for Big Five Personality detection in a Dutch corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
